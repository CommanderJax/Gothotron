{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'tacotron2'...\n",
      "remote: Enumerating objects: 403, done.\u001b[K\n",
      "remote: Total 403 (delta 0), reused 0 (delta 0), pack-reused 403\u001b[K\n",
      "Receiving objects: 100% (403/403), 2.69 MiB | 4.76 MiB/s, done.\n",
      "Resolving deltas: 100% (204/204), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NVIDIA/tacotron2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'bash'\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd tacotron2\n",
    "git submodule init; git submodule update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: six in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from gdown) (2.24.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from gdown) (4.48.2)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\users\\sebbi\\anaconda3\\lib\\site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9684 sha256=24a2fc3fa590d218c268363a00863ed70b07e7f998c589d12b68aaf798f90985\n",
      "  Stored in directory: c:\\users\\sebbi\\appdata\\local\\pip\\cache\\wheels\\e2\\62\\1e\\926d1ebe7b1e733c78d627fd288d01b83feaf67efc06e0e4c3\n",
      "Successfully built gdown\n",
      "Installing collected packages: gdown\n",
      "Successfully installed gdown-3.12.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "datadir='/datadir/tacotron2' # CHANGE THIS\n",
    "os.chdir('/datadir/tacotron2')# CHANGE THIS\n",
    "outdir = datadir + '/outdir'\n",
    "logsdirtemp = outdir + '/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed -i -- 's,DUMMY,wavs,g' filelists/*.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datadir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1dc060a010b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mitertools\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatadir\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/filelists/filelist_train.txt\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m            \u001b[0mlines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mislice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m            \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'datadir' is not defined"
     ]
    }
   ],
   "source": [
    "#let's see if we've got the right content inside\n",
    "import itertools\n",
    "with open(datadir + \"/filelists/filelist_train.txt\") as f:\n",
    "           lines=list(itertools.islice(f,5,10,1))\n",
    "           print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.17.2\n",
      "  Downloading numpy-1.17.2-cp36-cp36m-manylinux1_x86_64.whl (20.4 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.17.0\n",
      "    Uninstalling numpy-1.17.0:\n",
      "      Successfully uninstalled numpy-1.17.0\n",
      "Successfully installed numpy-1.17.2\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install  numpy==1.17.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/datadir/tacotron2/plotting_utils.py:2: UserWarning: \n",
      "This call to matplotlib.use() has no effect because the backend has already\n",
      "been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/opt/conda/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 612, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/platform/asyncio.py\", line 149, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 442, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/base_events.py\", line 1462, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/conda/lib/python3.6/asyncio/events.py\", line 145, in _run\n",
      "    self._callback(*self._args)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 365, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 545, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 306, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2867, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2895, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3072, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3263, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3343, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-2-58edc26e37cb>\", line 10, in <module>\n",
      "    import matplotlib.pylab as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pylab.py\", line 257, in <module>\n",
      "    from matplotlib import cbook, mlab, pyplot as plt\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py\", line 69, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  matplotlib.use(\"Agg\")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/datadir/tacotron2') # you might need/want to change this\n",
    "import time\n",
    "import argparse\n",
    "import math\n",
    "from numpy import finfo\n",
    "\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "from distributed import apply_gradient_allreduce\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import Tacotron2\n",
    "from data_utils import TextMelLoader, TextMelCollate\n",
    "from loss_function import Tacotron2Loss\n",
    "from logger import Tacotron2Logger\n",
    "from hparams import create_hparams\n",
    "\n",
    "def reduce_tensor(tensor, n_gpus):\n",
    "    rt = tensor.clone()\n",
    "    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n",
    "    rt /= n_gpus\n",
    "    return rt\n",
    "\n",
    "def init_distributed(hparams, n_gpus, rank, group_name):\n",
    "    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n",
    "    print(\"Initializing Distributed\")\n",
    "\n",
    "    # Set cuda device so everything is done on the right GPU.\n",
    "    torch.cuda.set_device(rank % torch.cuda.device_count())\n",
    "\n",
    "    # Initialize distributed communication\n",
    "    dist.init_process_group(\n",
    "        backend=hparams.dist_backend, init_method=hparams.dist_url,\n",
    "        world_size=n_gpus, rank=rank, group_name=group_name)\n",
    "\n",
    "    print(\"Done initializing distributed\")\n",
    "    \n",
    "    \n",
    "def prepare_dataloaders(hparams):\n",
    "    # Get data, data loaders and collate function ready\n",
    "    trainset = TextMelLoader(hparams.training_files, hparams)\n",
    "    valset = TextMelLoader(hparams.validation_files, hparams)\n",
    "    collate_fn = TextMelCollate(hparams.n_frames_per_step)\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        train_sampler = DistributedSampler(trainset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "\n",
    "    train_loader = DataLoader(trainset, num_workers=1, shuffle=shuffle,\n",
    "                              sampler=train_sampler,\n",
    "                              batch_size=hparams.batch_size, pin_memory=False,\n",
    "                              drop_last=True, collate_fn=collate_fn)\n",
    "    return train_loader, valset, collate_fn\n",
    "\n",
    "\n",
    "def prepare_directories_and_logger(output_directory, log_directory, rank):\n",
    "    if rank == 0:\n",
    "        if not os.path.isdir(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "            os.chmod(output_directory, 0o775)\n",
    "        logger = Tacotron2Logger(os.path.join(output_directory, log_directory))\n",
    "    else:\n",
    "        logger = None\n",
    "    return logger\n",
    "\n",
    "def load_model(hparams):\n",
    "    model = Tacotron2(hparams).cuda()\n",
    "    if hparams.fp16_run:\n",
    "        model.decoder.attention_layer.score_mask_value = finfo('float16').min\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def warm_start_model(checkpoint_path, model, ignore_layers):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Warm starting model from checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = checkpoint_dict['state_dict']\n",
    "    if len(ignore_layers) > 0:\n",
    "        model_dict = {k: v for k, v in model_dict.items()\n",
    "                      if k not in ignore_layers}\n",
    "        dummy_dict = model.state_dict()\n",
    "        dummy_dict.update(model_dict)\n",
    "        model_dict = dummy_dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    assert os.path.isfile(checkpoint_path)\n",
    "    print(\"Loading checkpoint '{}'\".format(checkpoint_path))\n",
    "    checkpoint_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint_dict['state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint_dict['optimizer'])\n",
    "    learning_rate = checkpoint_dict['learning_rate']\n",
    "    iteration = checkpoint_dict['iteration']\n",
    "    print(\"Loaded checkpoint '{}' from iteration {}\" .format(\n",
    "        checkpoint_path, iteration))\n",
    "    return model, optimizer, learning_rate, iteration\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, learning_rate, iteration, filepath):\n",
    "    print(\"Saving model and optimizer state at iteration {} to {}\".format(\n",
    "        iteration, filepath))\n",
    "    torch.save({'iteration': iteration,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'learning_rate': learning_rate}, filepath)\n",
    "    \n",
    "    \n",
    "def validate(model, criterion, valset, iteration, batch_size, n_gpus,\n",
    "             collate_fn, logger, distributed_run, rank):\n",
    "    \"\"\"Handles all the validation scoring and printing\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_sampler = DistributedSampler(valset) if distributed_run else None\n",
    "        val_loader = DataLoader(valset, sampler=val_sampler, num_workers=1,\n",
    "                                shuffle=False, batch_size=batch_size,\n",
    "                                pin_memory=False, collate_fn=collate_fn)\n",
    "\n",
    "        val_loss = 0.0\n",
    "        for i, batch in enumerate(val_loader):\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            if distributed_run:\n",
    "                reduced_val_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_val_loss = loss.item()\n",
    "            val_loss += reduced_val_loss\n",
    "        val_loss = val_loss / (i + 1)\n",
    "\n",
    "    model.train()\n",
    "    if rank == 0:\n",
    "        print(\"Validation loss {}: {:9f}  \".format(iteration, val_loss))\n",
    "        logger.log_validation(val_loss, model, y, y_pred, iteration)\n",
    "        %matplotlib inline\n",
    "        _, mel_outputs, gate_outputs, alignments = y_pred\n",
    "        idx = random.randint(0, alignments.size(0) - 1)\n",
    "        plot_alignment(alignments[idx].data.cpu().numpy().T)\n",
    "        \n",
    "def plot_alignment(alignment, info=None):\n",
    "    %matplotlib inline\n",
    "    fig, ax = plt.subplots(figsize=(int(alignment_graph_width/100), int(alignment_graph_height/100)))\n",
    "    im = ax.imshow(alignment, cmap='inferno', aspect='auto', origin='lower',\n",
    "                   interpolation='none')\n",
    "    ax.autoscale(enable=True, axis=\"y\", tight=True)\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    xlabel = 'Decoder timestep'\n",
    "    if info is not None:\n",
    "        xlabel += '\\n\\n' + info\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel('Encoder timestep')\n",
    "    plt.tight_layout()\n",
    "    fig.canvas.draw()\n",
    "    plt.show()        \n",
    "def train(output_directory, log_directory, checkpoint_path, warm_start, n_gpus,\n",
    "          rank, group_name, hparams):\n",
    "    \"\"\"Training and validation logging results to tensorboard and stdout\n",
    "\n",
    "    Params\n",
    "    ------\n",
    "    output_directory (string): directory to save checkpoints\n",
    "    log_directory (string) directory to save tensorboard logs\n",
    "    checkpoint_path(string): checkpoint path\n",
    "    n_gpus (int): number of gpus\n",
    "    rank (int): rank of current gpu\n",
    "    hparams (object): comma separated list of \"name=value\" pairs.\n",
    "    \"\"\"\n",
    "    if hparams.distributed_run:\n",
    "        init_distributed(hparams, n_gpus, rank, group_name)\n",
    "\n",
    "    torch.manual_seed(hparams.seed)\n",
    "    torch.cuda.manual_seed(hparams.seed)\n",
    "\n",
    "    model = load_model(hparams)\n",
    "    learning_rate = hparams.learning_rate\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,\n",
    "                                 weight_decay=hparams.weight_decay)\n",
    "\n",
    "    if hparams.fp16_run:\n",
    "        from apex import amp\n",
    "        model, optimizer = amp.initialize(\n",
    "            model, optimizer, opt_level='O2')\n",
    "\n",
    "    if hparams.distributed_run:\n",
    "        model = apply_gradient_allreduce(model)\n",
    "\n",
    "    criterion = Tacotron2Loss()\n",
    "\n",
    "    logger = prepare_directories_and_logger(\n",
    "        output_directory, log_directory, rank)\n",
    "\n",
    "    train_loader, valset, collate_fn = prepare_dataloaders(hparams)\n",
    "\n",
    "    # Load checkpoint if one exists\n",
    "    iteration = 0\n",
    "    epoch_offset = 0\n",
    "    if checkpoint_path is not None:\n",
    "        if warm_start:\n",
    "            model = warm_start_model(\n",
    "                checkpoint_path, model, hparams.ignore_layers)\n",
    "        else:\n",
    "            model, optimizer, _learning_rate, iteration = load_checkpoint(\n",
    "                checkpoint_path, model, optimizer)\n",
    "            if hparams.use_saved_learning_rate:\n",
    "                learning_rate = _learning_rate\n",
    "            iteration += 1  # next iteration is iteration + 1\n",
    "            epoch_offset = max(0, int(iteration / len(train_loader)))\n",
    "\n",
    "    model.train()\n",
    "    is_overflow = False\n",
    "    # ================ MAIN TRAINNIG LOOP! ===================\n",
    "    for epoch in range(epoch_offset, hparams.epochs):\n",
    "        print(\"Epoch: {}\".format(epoch))\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            start = time.perf_counter()\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = learning_rate\n",
    "\n",
    "            model.zero_grad()\n",
    "            x, y = model.parse_batch(batch)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y)\n",
    "            if hparams.distributed_run:\n",
    "                reduced_loss = reduce_tensor(loss.data, n_gpus).item()\n",
    "            else:\n",
    "                reduced_loss = loss.item()\n",
    "            if hparams.fp16_run:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            if hparams.fp16_run:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    amp.master_params(optimizer), hparams.grad_clip_thresh)\n",
    "                is_overflow = math.isnan(grad_norm)\n",
    "            else:\n",
    "                grad_norm = torch.nn.utils.clip_grad_norm_(\n",
    "                    model.parameters(), hparams.grad_clip_thresh)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if not is_overflow and rank == 0:\n",
    "                duration = time.perf_counter() - start\n",
    "                print(\"Train loss {} {:.6f} Grad Norm {:.6f} {:.2f}s/it\".format(\n",
    "                    iteration, reduced_loss, grad_norm, duration))\n",
    "                logger.log_training(\n",
    "                    reduced_loss, grad_norm, learning_rate, duration, iteration)\n",
    "\n",
    "            if not is_overflow and (iteration % hparams.iters_per_checkpoint == 0):\n",
    "                validate(model, criterion, valset, iteration,\n",
    "                         hparams.batch_size, n_gpus, collate_fn, logger,\n",
    "                         hparams.distributed_run, rank)\n",
    "                if rank == 0:\n",
    "                    checkpoint_path = os.path.join(\n",
    "                        output_directory, \"checkpoint_{}\".format(iteration))\n",
    "                    save_checkpoint(model, optimizer, learning_rate, iteration,\n",
    "                                    checkpoint_path)\n",
    "\n",
    "            iteration += 1\n",
    "            \n",
    "            \n",
    "alignment_graph_height = 600\n",
    "alignment_graph_width = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text import symbols\n",
    "warm_start=True\n",
    "n_gpus=1\n",
    "rank=0\n",
    "group_name=None\n",
    "hparams = create_hparams()\n",
    "hparams.epochs=5000\n",
    "hparams.iters_per_checkpoint=500\n",
    "hparams.seed=1234\n",
    "hparams.dynamic_loss_scaling=True\n",
    "hparams.fp16_run=False\n",
    "hparams.distributed_run=False\n",
    "hparams.dist_backend=\"nccl\"\n",
    "hparams.dist_url=\"tcp://localhost:54321\"\n",
    "hparams.cudnn_enabled=True\n",
    "hparams.cudnn_benchmark=False\n",
    "# hparams.ignore_layers=[]\n",
    "hparams.ignore_layers=['embedding.weight']\n",
    "        ################################\n",
    "        # Data Parameters             #\n",
    "        ################################\n",
    "hparams.load_mel_from_disk=False\n",
    "hparams.training_files = \"filelists/filelist_train.txt\"  # CHANGE THIS\n",
    "hparams.validation_files = \"filelists/filelist_validation.txt\" # CHANGE THIS\n",
    "hparams.text_cleaners=['english_cleaners'] # CHANGE this if training on non-english dataset (btw. this is more nuanced and involved, training on non english datasets requires more changes, for more information refer to the issues in NVIDIA/tacotron2 github.)\n",
    "\n",
    "        ################################\n",
    "        # Audio Parameters             #\n",
    "        ################################\n",
    "hparams.max_wav_value=32768.0\n",
    "hparams.sampling_rate=22050\n",
    "hparams.filter_length=1024\n",
    "hparams.hop_length=256\n",
    "hparams.win_length=1024\n",
    "hparams.n_mel_channels=80\n",
    "hparams.mel_fmin=0.0\n",
    "hparams.mel_fmax=8000.0\n",
    "\n",
    "        ################################\n",
    "        # Model Parameters             #\n",
    "        ################################\n",
    "hparams.n_symbols=len(symbols)\n",
    "hparams.symbols_embedding_dim=512\n",
    "\n",
    "        # Encoder parameters\n",
    "hparams.encoder_kernel_size=5\n",
    "hparams.encoder_n_convolutions=3\n",
    "hparams.encoder_embedding_dim=512\n",
    "\n",
    "        # Decoder parameters\n",
    "hparams.n_frames_per_step=1  # currently only 1 is supported\n",
    "hparams.decoder_rnn_dim=1024\n",
    "hparams.prenet_dim=256\n",
    "hparams.max_decoder_steps=1000\n",
    "hparams.gate_threshold=0.5\n",
    "hparams.p_attention_dropout=0.1\n",
    "hparams.p_decoder_dropout=0.1\n",
    "\n",
    "        # Attention parameters\n",
    "hparams.attention_rnn_dim=1024\n",
    "hparams.attention_dim=128\n",
    "\n",
    "        # Location Layer parameters\n",
    "hparams.attention_location_n_filters=32\n",
    "hparams.attention_location_kernel_size=31\n",
    "\n",
    "        # Mel-post processing network parameters\n",
    "hparams.postnet_embedding_dim=512\n",
    "hparams.postnet_kernel_size=5\n",
    "hparams.postnet_n_convolutions=5\n",
    "\n",
    "        ################################\n",
    "        # Optimization Hyperparameters #\n",
    "        ################################\n",
    "hparams.use_saved_learning_rate=False\n",
    "hparams.learning_rate=1e-3\n",
    "hparams.weight_decay=1e-6\n",
    "hparams.grad_clip_thresh=1.0\n",
    "hparams.batch_size=32 # CHANGE THIS, you might need to change this depending on your GPU model\n",
    "hparams.mask_padding=True  # set model's padded outputs to padded values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22050.0\n",
      "22050\n",
      "15699\n",
      "15699\n"
     ]
    }
   ],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "(sig, rate) = wav.read(\"PLACEHOLDER\") # CHANGE THIS, replace with path to one of your downsampled wavs to confirm everything looks good\n",
    "print(sig) # must be 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1a25ab820511>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbenchmark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn_benchmark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FP16 Run:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp16_run\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Dynamic Loss Scaling:\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdynamic_loss_scaling\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hparams' is not defined"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.enabled = hparams.cudnn_enabled\n",
    "torch.backends.cudnn.benchmark = hparams.cudnn_benchmark\n",
    "\n",
    "print(\"FP16 Run:\", hparams.fp16_run)\n",
    "print(\"Dynamic Loss Scaling:\", hparams.dynamic_loss_scaling)\n",
    "print(\"Distributed Run:\", hparams.distributed_run)\n",
    "print(\"cuDNN Enabled:\", hparams.cudnn_enabled)\n",
    "print(\"cuDNN Benchmark:\", hparams.cudnn_benchmark)\n",
    "\n",
    "output_directory = outdir # Location to save Checkpoints\n",
    "log_directory = outdir + '/logs' # Location to save Log files locally\n",
    "model_filename = \"nameless_hero_model\"\n",
    "checkpoint_path = output_directory+(r'/')+model_filename\n",
    "warm_start = True\n",
    "n_gpus = 1\n",
    "rank = 0\n",
    "group_name = None\n",
    "\n",
    "train(output_directory, log_directory, checkpoint_path,\n",
    "      warm_start, n_gpus, rank, group_name, hparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
